{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to create, train, validate and test the Neural Network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "\n",
    "from NN_config import Config\n",
    "from HD_DataLoader import *\n",
    "from NN_model import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the database and create the training, validation, and test set according to the first itteration as described in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = os.path.join(os.getcwd(), 'HD_data') \n",
    "# HD_class = HD(data_path = folder_path)\n",
    "# database = HD_class.generate_database()\n",
    "\n",
    "# # Creating the dataset\n",
    "# # The use of microphone data is set as well as current data can be set to True or False, and the domain in which they are used can be set to Frequency or Time\n",
    "# dataset = StatisticalFeaturesDataset(database, window_sec=2, selected_domain = {'Mic': 'freq', 'dSpace': 'time'}, Mic_bool = 'True', Curr_bool = 'False')\n",
    "\n",
    "# # Splitting the data into train, test, and validation sets\n",
    "# train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42, shuffle=True)\n",
    "# train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "# # Creating data loaders\n",
    "# batch_size = 32\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the database and create the training, validation, and test set according to the second itteration as described in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "folder_path = os.path.join(os.getcwd(), 'HD_data') \n",
    "HD_class = HD(data_path = folder_path)\n",
    "db_train, db_test = HD_class.generate_database_test_train()\n",
    "\n",
    "# The use of microphone data is set as well as current data can be set to True or False, and the domain in which they are used can be set to Frequency or Time\n",
    "dataset_train = StatisticalFeaturesDataset(db_train, window_sec=2, selected_domain = {'Mic': 'freq', 'dSpace': 'time'}, Mic_bool = True, Curr_bool = False)\n",
    "train_dataset, val_dataset = train_test_split(dataset_train, test_size=0.1, random_state=42, shuffle=True)\n",
    "test_dataset = StatisticalFeaturesDataset(db_test, window_sec=2, selected_domain = {'Mic': 'freq', 'dSpace': 'time'}, Mic_bool = True, Curr_bool = False)\n",
    "\n",
    "# Creating data loaders\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "input_dim = train_features.shape[1]\n",
    "print('The input dimension: ' + str(input_dim))\n",
    "print('The amound of batches in the train loaders: ' + str(len(train_dataloader)))\n",
    "      \n",
    "cfg = Config()\n",
    "\n",
    "# Initialize network\n",
    "model = Classifier(input_dim)\n",
    "model.train()\n",
    "if cfg.enable_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "summary(model,input_size=(batch_size,input_dim))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "i = 0\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    \n",
    "    #Training phase\n",
    "    model.train()  # Set the model in training mode\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.float().reshape(-1,1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model in evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "            for i, data in enumerate(val_dataloader,0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                labels = labels.float().reshape(-1,1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                predicted = (outputs >= 0.5).squeeze().long()\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels.squeeze()).sum().item()\n",
    "\n",
    "    # Print training validation metrics\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_acc = correct / total * 100\n",
    "    train_loss = epoch_train_loss / len(train_dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{cfg.num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "print(\"Finished training.\")\n",
    "save_path = 'model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"Saved trained model as {}.\".format(save_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, cfg.num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, cfg.num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# track false positive rate and false negative rate\n",
    "false_positive = 0\n",
    "true_positive = 0\n",
    "false_negative = 0\n",
    "true_negative = 0\n",
    "\n",
    "print(\"Starting evaluation with the test set...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader,0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.float().reshape(-1,1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        predicted = (outputs >= 0.5).squeeze().long()\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.squeeze()).sum().item()\n",
    "        \n",
    "        for i in range(len(predicted)):\n",
    "                    if predicted[i] == 1 and labels[i] == 0:\n",
    "                        false_positive += 1\n",
    "                    elif predicted[i] == 0 and labels[i] == 1:\n",
    "                        false_negative += 1\n",
    "                    elif predicted[i] == 1 and labels[i] == 1:\n",
    "                        true_positive += 1\n",
    "                    elif predicted[i] == 0 and labels[i] == 0:\n",
    "                        true_negative += 1\n",
    "\n",
    "print(\"Accuracy of the network on the {} test images: {} %\".format(total, round((correct/total*100), 1)) )\n",
    "print(\"Correct predictions: {}\".format(correct))\n",
    "print(\"Total predictions: {}\".format(total))\n",
    "\n",
    "false_positive_rate = false_positive / (false_positive + true_negative)\n",
    "false_negative_rate = false_negative / (false_negative + true_positive)\n",
    "\n",
    "print(\"False positive rate: \" + str(false_positive_rate))\n",
    "print(\"False negative rate: \" + str(false_negative_rate))\n",
    "print(\"Percentage of positively predicted samples: \" + str((false_positive + true_positive) / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Course_5ARIP10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
